{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DWD CDC Time Series, Merge with Station Description and Append \n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) from the DWD Climate Data Center in such a way that it can be used with the **QGIS time manager extension**. \n",
    "\n",
    "This extension allows to filter an attribute table of a vector layer (e.g. points representing precipitation stations plus precipitation data) with a time stamp column. The extension limits the attribute table to the records matching the particular time stamp provided by the time manager extension (e.g. by the user moving the time slider). This selected subset of the attribute table is then used to change the sympology of the vector layer according to the variable of interest (e.g. precipitation rate).\n",
    "\n",
    "The QGIS time manager extension approach is a bit brute force, because each individual measurement at a station at a given time is one feature (row in the table), i.e. a time series at station X with hourly resolution for a day (24 values) entails 24 different features with the same station id and the corresponding coordinates but different times. As of now this 1:n relationship can only be realized by importing a CSV file with the according structure. \n",
    "\n",
    "(At least I was not able to generate the required view on a 1:n relationship by merging a point vector layer with precipitation station locations and an imported CSV time series table.)\n",
    "\n",
    "The final data format is a concatenation of time series together with geographic location in 2D (e.g. lat, lon). The required data format looks principly like this:\n",
    "\n",
    "\n",
    "| station_id |        name        |   lat   |   lon  |        meas_time       | prec_rate |\n",
    "|:----------:|:------------------:|:-------:|:------:|:----------------------:|:---------:|\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T08:00:00UTC |       1.5 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T09:00:00UTC |       1.7 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T10:00:00UTC |       0.1 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T08:00:00UTC |       0.8 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T09:00:00UTC |       0.4 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T10:00:00UTC |       0.0 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "\n",
    "\n",
    "(Table generated with https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "To achieve this the precipitation time series (station_id, meas_time, prec_rate) have to be merged with the station metadata (station_id, lat, lon) coming from the a CSV file generated in an earlier activity. We use Pandas to read, join and append the data to generate the final CSV file to be imported as a point layer to QGIS. \n",
    "\n",
    "This final data format is far from being optimal because of large size and highly redundant information. This is a challenge for QGIS which loses responsiveness with large data. To jsut show the principle it is advisable to limit to size of the problem. \n",
    "\n",
    "The following filters (selection criteria) are applied:\n",
    "\n",
    "  * Precipitation stations in NRW only (approx. 127 stations) \n",
    "  * Hourly precipitation data\n",
    "  * Time interval from 2018-12-01 to last date in precipitation data set \n",
    "  \n",
    "Still: 40 days * 24 hrs / day * 127 stations = 121920 records leading to 121920 features in a point layer in QGIS. \n",
    "\n",
    "In fact, the resulting number of records is arround 91000. The reason might be that not all stations in the station list have time series. This has to be checked carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTP Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "#topic_dir = \"/hourly/precipitation/recent/\"\n",
    "#topic_dir = \"/daily/more_precip/historical/\"\n",
    "topic_dir = \"/daily/soil_temperature/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "#ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "#temprature \n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate\"\n",
    "\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallel merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/original/DWD/\n",
      "data/original/DWD//daily/soil_temperature/historical/\n",
      "data/original/DWD//daily/soil_temperature/historical/\n",
      "\n",
      "data/generated/DWD/\n",
      "data/generated/DWD//daily/soil_temperature/historical/\n",
      "data/generated/DWD//daily/soil_temperature/historical/\n",
      "data/generated/DWD//daily/soil_temperature/historical/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Grab File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabFile(ftpfullname,localfullname):\n",
    "    try:\n",
    "        ret = ftp.cwd(\".\") # A dummy action to chack the connection and to provoke an exception if necessary.\n",
    "        localfile = open(localfullname, 'wb')\n",
    "        ftp.retrbinary('RETR ' + ftpfullname, localfile.write, 1024)\n",
    "        localfile.close()\n",
    "    \n",
    "    except ftplib.error_perm:\n",
    "        print(\"FTP ERROR. Operation not permitted. File not found?\")\n",
    "\n",
    "    except ftplib.error_temp:\n",
    "        print(\"FTP ERROR. Timeout.\")\n",
    "\n",
    "    except ConnectionAbortedError:\n",
    "        print(\"FTP ERROR. Connection aborted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def gen_df_from_ftp_dir_listing(ftp, ftpdir):\n",
    "    lines = []\n",
    "    flist = []\n",
    "    try:    \n",
    "        res = ftp.retrlines(\"LIST \"+ftpdir, lines.append)\n",
    "    except:\n",
    "        print(\"Error: ftp.retrlines() failed. ftp timeout? Reconnect!\")\n",
    "        return\n",
    "        \n",
    "    if len(lines) == 0:\n",
    "        print(\"Error: ftp dir is empty\")\n",
    "        return\n",
    "    \n",
    "    for line in lines:\n",
    "#        print(line)\n",
    "        [ftype, fsize, fname] = [line[0:1], int(line[31:42]), line[56:]]\n",
    "\n",
    "        \n",
    "        fext = os.path.splitext(fname)[-1]\n",
    "        \n",
    "        if fext == \".zip\":\n",
    "            station_id = int(fname.split(\"_\")[2])\n",
    "        else:\n",
    "            station_id = -1 \n",
    "        \n",
    "        flist.append([station_id, fname, fext, fsize, ftype])\n",
    "        \n",
    "        \n",
    "\n",
    "    df_ftpdir = pd.DataFrame(flist,columns=[\"station_id\", \"name\", \"ext\", \"size\", \"type\"])\n",
    "    return(df_ftpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>BESCHREIBUNG_obsgermany_climate_daily_soil_tem...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>69925</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>DESCRIPTION_obsgermany_climate_daily_soil_temp...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>69373</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>EB_Tageswerte_Beschreibung_Stationen.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>99393</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>tageswerte_EB_00003_19510101_20110331_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>218207</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>tageswerte_EB_00044_19810101_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>136119</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>tageswerte_EB_00052_19760101_20011231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>98089</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>71</td>\n",
       "      <td>tageswerte_EB_00071_19880701_20031231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>58069</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>tageswerte_EB_00072_19870101_19950531_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>35119</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>tageswerte_EB_00078_19810101_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>133354</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>tageswerte_EB_00091_19920501_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>84507</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                                               name   ext  \\\n",
       "0          -1  BESCHREIBUNG_obsgermany_climate_daily_soil_tem...  .pdf   \n",
       "1          -1  DESCRIPTION_obsgermany_climate_daily_soil_temp...  .pdf   \n",
       "2          -1           EB_Tageswerte_Beschreibung_Stationen.txt  .txt   \n",
       "3           3     tageswerte_EB_00003_19510101_20110331_hist.zip  .zip   \n",
       "4          44     tageswerte_EB_00044_19810101_20181231_hist.zip  .zip   \n",
       "5          52     tageswerte_EB_00052_19760101_20011231_hist.zip  .zip   \n",
       "6          71     tageswerte_EB_00071_19880701_20031231_hist.zip  .zip   \n",
       "7          72     tageswerte_EB_00072_19870101_19950531_hist.zip  .zip   \n",
       "8          78     tageswerte_EB_00078_19810101_20181231_hist.zip  .zip   \n",
       "9          91     tageswerte_EB_00091_19920501_20181231_hist.zip  .zip   \n",
       "\n",
       "     size type  \n",
       "0   69925    -  \n",
       "1   69373    -  \n",
       "2   99393    -  \n",
       "3  218207    -  \n",
       "4  136119    -  \n",
       "5   98089    -  \n",
       "6   58069    -  \n",
       "7   35119    -  \n",
       "8  133354    -  \n",
       "9   84507    -  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ftpdir.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>tageswerte_EB_00003_19510101_20110331_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>218207</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>tageswerte_EB_00044_19810101_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>136119</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>tageswerte_EB_00052_19760101_20011231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>98089</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>tageswerte_EB_00071_19880701_20031231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>58069</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>tageswerte_EB_00072_19870101_19950531_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>35119</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>tageswerte_EB_00078_19810101_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>133354</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>tageswerte_EB_00091_19920501_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>84507</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>tageswerte_EB_00125_20010403_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>23630</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>tageswerte_EB_00129_19960701_20061231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>38445</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>tageswerte_EB_00131_20040914_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>50199</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      name   ext    size type\n",
       "station_id                                                                   \n",
       "3           tageswerte_EB_00003_19510101_20110331_hist.zip  .zip  218207    -\n",
       "44          tageswerte_EB_00044_19810101_20181231_hist.zip  .zip  136119    -\n",
       "52          tageswerte_EB_00052_19760101_20011231_hist.zip  .zip   98089    -\n",
       "71          tageswerte_EB_00071_19880701_20031231_hist.zip  .zip   58069    -\n",
       "72          tageswerte_EB_00072_19870101_19950531_hist.zip  .zip   35119    -\n",
       "78          tageswerte_EB_00078_19810101_20181231_hist.zip  .zip  133354    -\n",
       "91          tageswerte_EB_00091_19920501_20181231_hist.zip  .zip   84507    -\n",
       "125         tageswerte_EB_00125_20010403_20181231_hist.zip  .zip   23630    -\n",
       "129         tageswerte_EB_00129_19960701_20061231_hist.zip  .zip   38445    -\n",
       "131         tageswerte_EB_00131_20040914_20181231_hist.zip  .zip   50199    -"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EB_Tageswerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(station_fname)\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabFile: \n",
      "From: /climate_environment/CDC/observations_germany/climate/daily/soil_temperature/historical/EB_Tageswerte_Beschreibung_Stationen.txt\n",
      "To:   data/original/DWD//daily/soil_temperature/historical/EB_Tageswerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"grabFile: \")\n",
    "print(\"From: \" + ftp_dir + station_fname)\n",
    "print(\"To:   \" + local_ftp_station_dir + station_fname)\n",
    "grabFile(ftp_dir + station_fname, local_ftp_station_dir + station_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract column names. They are in German (de)\n",
    "# We have to use codecs because of difficulties with character encoding (German Umlaute)\n",
    "import codecs\n",
    "\n",
    "def station_desc_txt_to_csv(txtfile, csvfile):\n",
    "    file = codecs.open(txtfile,\"r\",\"utf-8\")\n",
    "    r = file.readline()\n",
    "    file.close()\n",
    "    colnames_de = r.split()\n",
    "    colnames_de\n",
    "    \n",
    "    translate = \\\n",
    "    {'Stations_id':'station_id',\n",
    "     'von_datum':'date_from',\n",
    "     'bis_datum':'date_to',\n",
    "     'Stationshoehe':'altitude',\n",
    "     'geoBreite': 'latitude',\n",
    "     'geoLaenge': 'longitude',\n",
    "     'Stationsname':'name',\n",
    "     'Bundesland':'state'}\n",
    "    \n",
    "    colnames_en = [translate[h] for h in colnames_de]\n",
    "    \n",
    "    # Skip the first two rows and set the column names.\n",
    "    df = pd.read_fwf(txtfile,skiprows=2,names=colnames_en, parse_dates=[\"date_from\",\"date_to\"],index_col = 0)\n",
    "    \n",
    "    # write csv\n",
    "    df.to_csv(csvfile, sep = \";\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>202</td>\n",
       "      <td>50.7827</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>44</td>\n",
       "      <td>52.9336</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1976-01-01</td>\n",
       "      <td>2001-12-31</td>\n",
       "      <td>46</td>\n",
       "      <td>53.6623</td>\n",
       "      <td>10.1990</td>\n",
       "      <td>Ahrensburg-Wulfsdorf</td>\n",
       "      <td>Schleswig-Holstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1988-07-01</td>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>759</td>\n",
       "      <td>48.2156</td>\n",
       "      <td>8.9784</td>\n",
       "      <td>Albstadt-Badkap</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1987-01-01</td>\n",
       "      <td>1995-05-31</td>\n",
       "      <td>794</td>\n",
       "      <td>48.2766</td>\n",
       "      <td>9.0001</td>\n",
       "      <td>Albstadt-Onstmettingen</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "3          1951-01-01 2011-03-31       202   50.7827     6.0941   \n",
       "44         1981-01-01 2020-02-28        44   52.9336     8.2370   \n",
       "52         1976-01-01 2001-12-31        46   53.6623    10.1990   \n",
       "71         1988-07-01 2003-12-31       759   48.2156     8.9784   \n",
       "72         1987-01-01 1995-05-31       794   48.2766     9.0001   \n",
       "\n",
       "                              name                state  \n",
       "station_id                                               \n",
       "3                           Aachen  Nordrhein-Westfalen  \n",
       "44                    Großenkneten        Niedersachsen  \n",
       "52            Ahrensburg-Wulfsdorf   Schleswig-Holstein  \n",
       "71                 Albstadt-Badkap    Baden-Württemberg  \n",
       "72          Albstadt-Onstmettingen    Baden-Württemberg  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Stations Located in NRW from Station Description Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  603,   617,  1078,  1246,  1303,  1590,  1766,  2483,  2497,\n",
       "             2667,  2947,  3028,  3031,  3098,  3540,  3623,  4063,  4127,\n",
       "             4371,  5347,  5480,  6197,  7106,  7330,  7374, 15000],\n",
       "           dtype='int64', name='station_id')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "d1 = datetime.datetime(2018, 12, 31) \n",
    "#df_stations['date_to']==d1\n",
    "station_ids_selected = df_stations[df_stations['state'].str.contains(\"Nordrhein\") & (df_stations['date_to']>=d1)].index\n",
    "station_ids_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "#isNRW = dfStations['state'] == \"Nordrhein-Westfalen\"\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "#isOperational = dfStations['date_to'] == dfStations.date_to.max() \n",
    "\n",
    "# select on both conditions\n",
    "#dfNRW = dfStations[isNRW & isOperational]\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "#dfNRW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>tageswerte_EB_00003_19510101_20110331_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>218207</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>tageswerte_EB_00044_19810101_20181231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>136119</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>tageswerte_EB_00052_19760101_20011231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>98089</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>tageswerte_EB_00071_19880701_20031231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>58069</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>tageswerte_EB_00072_19870101_19950531_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>35119</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      name   ext    size type\n",
       "station_id                                                                   \n",
       "3           tageswerte_EB_00003_19510101_20110331_hist.zip  .zip  218207    -\n",
       "44          tageswerte_EB_00044_19810101_20181231_hist.zip  .zip  136119    -\n",
       "52          tageswerte_EB_00052_19760101_20011231_hist.zip  .zip   98089    -\n",
       "71          tageswerte_EB_00071_19880701_20031231_hist.zip  .zip   58069    -\n",
       "72          tageswerte_EB_00072_19870101_19950531_hist.zip  .zip   35119    -"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server\n",
    "\n",
    "Problem: Not all stations listed in the station description file are associated with a time series (zip file)! The stations in the description file and the set of stations whoch are TS data provided for (zip files) do not match perfectly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tageswerte_EB_00603_20000701_20181231_hist.zip\n",
      "tageswerte_EB_00617_20030122_20181231_hist.zip\n",
      "tageswerte_EB_01078_19860101_20181231_hist.zip\n",
      "tageswerte_EB_01246_20150801_20181231_hist.zip\n",
      "tageswerte_EB_01303_19510101_20181231_hist.zip\n",
      "tageswerte_EB_01590_19810101_20181231_hist.zip\n",
      "tageswerte_EB_01766_19891001_20181231_hist.zip\n",
      "tageswerte_EB_02483_19810101_20181231_hist.zip\n",
      "tageswerte_EB_02497_20030604_20181231_hist.zip\n",
      "tageswerte_EB_02667_19610101_20181231_hist.zip\n",
      "tageswerte_EB_02947_20061001_20181231_hist.zip\n",
      "tageswerte_EB_03028_19810101_20181231_hist.zip\n",
      "tageswerte_EB_03031_19801201_20181231_hist.zip\n",
      "tageswerte_EB_03098_19931222_20181231_hist.zip\n",
      "tageswerte_EB_03540_20040818_20181231_hist.zip\n",
      "tageswerte_EB_03623_20010706_20181231_hist.zip\n",
      "tageswerte_EB_04063_20021022_20181231_hist.zip\n",
      "tageswerte_EB_04127_20041213_20181231_hist.zip\n",
      "tageswerte_EB_04371_19810101_20181231_hist.zip\n",
      "tageswerte_EB_05347_19880201_20181231_hist.zip\n",
      "tageswerte_EB_05480_20030910_20181231_hist.zip\n",
      "tageswerte_EB_06197_20001013_20181231_hist.zip\n",
      "tageswerte_EB_07106_20060901_20181231_hist.zip\n",
      "tageswerte_EB_07330_20050711_20181231_hist.zip\n",
      "tageswerte_EB_07374_20060301_20181231_hist.zip\n",
      "tageswerte_EB_15000_20110401_20181231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "# Add the names of the zip files only to a list. \n",
    "local_zip_list = []\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join (Merge) the Time Series Columns\n",
    "\n",
    "https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_ts_to_df(fname):\n",
    "    \n",
    "   # dateparse = lambda dates: [pd.datetime.strptime(str(d), '%Y%m%d%H') for d in dates]\n",
    "    dateparse = lambda dates: [pd.datetime.strptime(str(d), '%Y%m%d') for d in dates]\n",
    "\n",
    "    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "\n",
    "    #df = pd.read_csv(fname, delimiter=\";\", encoding=\"iso8859_2\",\\\n",
    "    #             index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse)\n",
    "    \n",
    "    # https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n",
    "\n",
    "    # Column headers: remove leading blanks (strip), replace \" \" with \"_\", and convert to lower case.\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_merge():\n",
    "   \n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            \n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = prec_ts_to_df(myfile)\n",
    "              \n",
    "                s = dftmp[\"stations_id\"].to_frame()\n",
    "                # outer merge.\n",
    "                df = pd.merge(df, s, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    #df.index.names = [\"year\"]\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00603_20000701_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20000701_20181231_00603.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00617_20030122_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20030122_20181231_00617.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01078_19860101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19860101_20181231_01078.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01246_20150801_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20150801_20181231_01246.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01303_19510101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20181231_01303.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01590_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_01590.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01766_19891001_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19891001_20181231_01766.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02483_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_02483.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02497_20030604_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20030604_20181231_02497.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02667_19610101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19610101_20181231_02667.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02947_20061001_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20061001_20181231_02947.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03028_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_03028.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03031_19801201_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19801201_20181231_03031.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03098_19931222_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19931222_20181231_03098.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03540_20040818_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20040818_20181231_03540.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03623_20010706_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20010706_20181231_03623.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_04063_20021022_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20021022_20181231_04063.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_04127_20041213_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20041213_20181231_04127.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_04371_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_04371.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05347_19880201_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19880201_20181231_05347.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05480_20030910_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20030910_20181231_05480.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_06197_20001013_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20001013_20181231_06197.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_07106_20060901_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20060901_20181231_07106.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_07330_20050711_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20050711_20181231_07330.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_07374_20060301_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20060301_20181231_07374.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_15000_20110401_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20110401_20181231_15000.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merged_ts = ts_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>...</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1951-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stations_id_x  stations_id_y  stations_id_x  stations_id_y  \\\n",
       "time                                                                     \n",
       "1951-01-01            NaN            NaN            NaN            NaN   \n",
       "1951-01-02            NaN            NaN            NaN            NaN   \n",
       "1951-01-03            NaN            NaN            NaN            NaN   \n",
       "1951-01-04            NaN            NaN            NaN            NaN   \n",
       "1951-01-05            NaN            NaN            NaN            NaN   \n",
       "\n",
       "            stations_id_x  stations_id_y  stations_id_x  stations_id_y  \\\n",
       "time                                                                     \n",
       "1951-01-01         1303.0            NaN            NaN            NaN   \n",
       "1951-01-02         1303.0            NaN            NaN            NaN   \n",
       "1951-01-03         1303.0            NaN            NaN            NaN   \n",
       "1951-01-04         1303.0            NaN            NaN            NaN   \n",
       "1951-01-05         1303.0            NaN            NaN            NaN   \n",
       "\n",
       "            stations_id_x  stations_id_y  ...  stations_id_x  stations_id_y  \\\n",
       "time                                      ...                                 \n",
       "1951-01-01            NaN            NaN  ...            NaN            NaN   \n",
       "1951-01-02            NaN            NaN  ...            NaN            NaN   \n",
       "1951-01-03            NaN            NaN  ...            NaN            NaN   \n",
       "1951-01-04            NaN            NaN  ...            NaN            NaN   \n",
       "1951-01-05            NaN            NaN  ...            NaN            NaN   \n",
       "\n",
       "            stations_id_x  stations_id_y  stations_id_x  stations_id_y  \\\n",
       "time                                                                     \n",
       "1951-01-01            NaN            NaN            NaN            NaN   \n",
       "1951-01-02            NaN            NaN            NaN            NaN   \n",
       "1951-01-03            NaN            NaN            NaN            NaN   \n",
       "1951-01-04            NaN            NaN            NaN            NaN   \n",
       "1951-01-05            NaN            NaN            NaN            NaN   \n",
       "\n",
       "            stations_id_x  stations_id_y  stations_id_x  stations_id_y  \n",
       "time                                                                    \n",
       "1951-01-01            NaN            NaN            NaN            NaN  \n",
       "1951-01-02            NaN            NaN            NaN            NaN  \n",
       "1951-01-03            NaN            NaN            NaN            NaN  \n",
       "1951-01-04            NaN            NaN            NaN            NaN  \n",
       "1951-01-05            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts.to_csv(local_ts_merged_dir + \"ts_merged.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_append():\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    i=0;\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = prec_ts_to_df(myfile)\n",
    "                dftmp=dftmp.loc[(dftmp.index>='2015-06-10 00:00:00')&(dftmp.index<='2018-05-11 00:00:00')]\n",
    "                dftmp = dftmp.merge(df_stations,how=\"inner\",left_on=\"stations_id\",right_on=\"station_id\",right_index=True)\n",
    "\n",
    "                df = df.append(dftmp)\n",
    "        \n",
    "   \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00603_20000701_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20000701_20181231_00603.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00617_20030122_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20030122_20181231_00617.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01078_19860101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19860101_20181231_01078.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01246_20150801_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20150801_20181231_01246.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01303_19510101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20181231_01303.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01590_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_01590.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_01766_19891001_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19891001_20181231_01766.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02483_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_02483.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02497_20030604_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20030604_20181231_02497.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02667_19610101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19610101_20181231_02667.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02947_20061001_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20061001_20181231_02947.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03028_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_03028.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03031_19801201_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19801201_20181231_03031.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03098_19931222_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19931222_20181231_03098.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03540_20040818_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20040818_20181231_03540.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03623_20010706_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20010706_20181231_03623.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_04063_20021022_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20021022_20181231_04063.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_04127_20041213_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20041213_20181231_04127.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_04371_19810101_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19810101_20181231_04371.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05347_19880201_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19880201_20181231_05347.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05480_20030910_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20030910_20181231_05480.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_06197_20001013_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20001013_20181231_06197.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_07106_20060901_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20060901_20181231_07106.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_07330_20050711_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20050711_20181231_07330.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_07374_20060301_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20060301_20181231_07374.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_15000_20110401_20181231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_20110401_20181231_15000.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_appended_ts = ts_append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stations_id</th>\n",
       "      <th>qn_2</th>\n",
       "      <th>v_te002m</th>\n",
       "      <th>v_te005m</th>\n",
       "      <th>v_te010m</th>\n",
       "      <th>v_te020m</th>\n",
       "      <th>v_te050m</th>\n",
       "      <th>eor</th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mess_datum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2015-06-10</td>\n",
       "      <td>603</td>\n",
       "      <td>3</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>17.1</td>\n",
       "      <td>eor</td>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>144</td>\n",
       "      <td>50.729</td>\n",
       "      <td>7.2047</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>603</td>\n",
       "      <td>3</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>21.9</td>\n",
       "      <td>20.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>eor</td>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>144</td>\n",
       "      <td>50.729</td>\n",
       "      <td>7.2047</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>603</td>\n",
       "      <td>3</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>22.6</td>\n",
       "      <td>21.3</td>\n",
       "      <td>18.4</td>\n",
       "      <td>eor</td>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>144</td>\n",
       "      <td>50.729</td>\n",
       "      <td>7.2047</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-06-13</td>\n",
       "      <td>603</td>\n",
       "      <td>3</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>20.6</td>\n",
       "      <td>20.7</td>\n",
       "      <td>20.5</td>\n",
       "      <td>18.8</td>\n",
       "      <td>eor</td>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>144</td>\n",
       "      <td>50.729</td>\n",
       "      <td>7.2047</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2015-06-14</td>\n",
       "      <td>603</td>\n",
       "      <td>3</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>20.4</td>\n",
       "      <td>19.8</td>\n",
       "      <td>18.4</td>\n",
       "      <td>eor</td>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>144</td>\n",
       "      <td>50.729</td>\n",
       "      <td>7.2047</td>\n",
       "      <td>Königswinter-Heiderhof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stations_id  qn_2  v_te002m  v_te005m  v_te010m  v_te020m  \\\n",
       "mess_datum                                                              \n",
       "2015-06-10          603     3    -999.0      20.7      20.0      18.8   \n",
       "2015-06-11          603     3    -999.0      22.7      21.9      20.4   \n",
       "2015-06-12          603     3    -999.0      23.3      22.6      21.3   \n",
       "2015-06-13          603     3    -999.0      20.6      20.7      20.5   \n",
       "2015-06-14          603     3    -999.0      20.7      20.4      19.8   \n",
       "\n",
       "            v_te050m  eor  date_from    date_to  altitude  latitude  \\\n",
       "mess_datum                                                            \n",
       "2015-06-10      17.1  eor 2000-07-01 2020-02-23       144    50.729   \n",
       "2015-06-11      17.6  eor 2000-07-01 2020-02-23       144    50.729   \n",
       "2015-06-12      18.4  eor 2000-07-01 2020-02-23       144    50.729   \n",
       "2015-06-13      18.8  eor 2000-07-01 2020-02-23       144    50.729   \n",
       "2015-06-14      18.4  eor 2000-07-01 2020-02-23       144    50.729   \n",
       "\n",
       "            longitude                    name                state  \n",
       "mess_datum                                                          \n",
       "2015-06-10     7.2047  Königswinter-Heiderhof  Nordrhein-Westfalen  \n",
       "2015-06-11     7.2047  Königswinter-Heiderhof  Nordrhein-Westfalen  \n",
       "2015-06-12     7.2047  Königswinter-Heiderhof  Nordrhein-Westfalen  \n",
       "2015-06-13     7.2047  Königswinter-Heiderhof  Nordrhein-Westfalen  \n",
       "2015-06-14     7.2047  Königswinter-Heiderhof  Nordrhein-Westfalen  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_appended_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts.to_csv(local_ts_appended_dir + \"ts_appended.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603\n",
      "617\n",
      "1078\n",
      "1246\n",
      "1303\n",
      "1590\n",
      "1766\n",
      "2483\n",
      "2497\n",
      "2667\n",
      "2947\n",
      "3028\n",
      "3031\n",
      "3098\n",
      "3540\n",
      "3623\n",
      "4063\n",
      "4127\n",
      "4371\n",
      "5347\n",
      "5480\n",
      "6197\n",
      "7106\n",
      "7330\n",
      "7374\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "stations=[]\n",
    "latitudes=[]\n",
    "longitudes=[]\n",
    "temperature_values=[]\n",
    "years=[]\n",
    "\n",
    "\n",
    "stations_ids=df_appended_ts['stations_id'].unique()\n",
    "for st in stations_ids:\n",
    "    print(st)\n",
    "\n",
    "    rows2016 = df_appended_ts[df_appended_ts.index <= datetime.datetime(2016, 6, 10)]\n",
    "    rows2016= rows2016[rows2016.stations_id==st]\n",
    "    latitude=\"\"\n",
    "    longitude=\"\"\n",
    "    if (len(rows2016)>0):\n",
    "        latitude=rows2016.iloc[0].latitude\n",
    "        longitude=rows2016.iloc[0].longitude\n",
    "    stations.append(st)\n",
    "    latitudes.append(latitude)\n",
    "    longitudes.append(longitude)\n",
    "    years.append(2016)\n",
    "    temperature_values.append(rows2016['v_te020m'].mean())\n",
    "    \n",
    "    rows2017 = df_appended_ts[df_appended_ts.index >= datetime.datetime(2016, 5, 26)]\n",
    "    rows2017 = rows2017[rows2017.index <= datetime.datetime(2017, 5, 26)]   \n",
    "    rows2017= rows2017[rows2017.stations_id==st]\n",
    "    if (len(rows2017)>0 and (latitude==\"\")):\n",
    "        latitude=rows2017.iloc[0].latitude\n",
    "        longitude=rows2017.iloc[0].longitude\n",
    "    stations.append(st)\n",
    "    latitudes.append(latitude)\n",
    "    longitudes.append(longitude)\n",
    "    years.append(2017)\n",
    "    temperature_values.append(rows2017['v_te020m'].mean())\n",
    "    \n",
    "    \n",
    "    rows2018 = df_appended_ts[df_appended_ts.index >= datetime.datetime(2017, 5, 11)]\n",
    "    rows2018 = rows2018[rows2018.index <= datetime.datetime(2018, 5, 11)]   \n",
    "    rows2018= rows2018[rows2018.stations_id==st]\n",
    "    if (len(rows2018)>0 and (latitude==\"\")):\n",
    "        latitude=rows2018.iloc[0].latitude\n",
    "        longitude=rows2018.iloc[0].longitude\n",
    "    stations.append(st)\n",
    "    latitudes.append(latitude)\n",
    "    longitudes.append(longitude)\n",
    "    years.append(2018)\n",
    "    temperature_values.append(rows2018['v_te020m'].mean())\n",
    "\n",
    "\n",
    "\n",
    "    temperature = {'stationids':stations,\n",
    "        'year':  years,\n",
    "        'at': temperature_values,\n",
    "        'lat':latitudes,\n",
    "        'long':longitudes \n",
    "                \n",
    "        }\n",
    "\n",
    "\n",
    "temperature_data_frame = pd.DataFrame(temperature, columns = ['stationids', 'year', 'at','lat','long'])\n",
    "temperature_data_frame.to_csv(local_ts_appended_dir + \"temperature.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! jupyter nbconvert --to html your_notebook_name.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
